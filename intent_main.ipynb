{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29a49daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c7d1da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('intentclass.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcbdc16f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>INTENTS</th>\n",
       "      <th>QUERIES</th>\n",
       "      <th>translated_text</th>\n",
       "      <th>Transliteration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>atis_flight</td>\n",
       "      <td>i want to fly from boston at 838 am and arriv...</td>\n",
       "      <td>मैं सुबह 838 बजे बोस्टन से उड़ान भरना चाहता हू...</td>\n",
       "      <td>maiM subaha 838 baje bosTana se u.DAna bharanA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>atis_flight</td>\n",
       "      <td>what flights are available from pittsburgh to...</td>\n",
       "      <td>गुरुवार सुबह पिट्सबर्ग से बाल्टीमोर तक कौन सी ...</td>\n",
       "      <td>guruvAra subaha piTsabarga se bAlTImora taka k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>atis_flight_time</td>\n",
       "      <td>what is the arrival time in san francisco for...</td>\n",
       "      <td>वाशिंगटन छोड़कर 755 बजे उड़ान के लिए सैन फ्रां...</td>\n",
       "      <td>vAshiMgaTana Cho.Dakara 755 baje u.DAna ke lie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>atis_airfare</td>\n",
       "      <td>cheapest airfare from tacoma to orlando</td>\n",
       "      <td>टैकोमा से ऑरलैंडो तक सबसे सस्ता विमान किराया</td>\n",
       "      <td>TaikomA se ऑralaiMDo taka sabase sastA vimAna ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>atis_airfare</td>\n",
       "      <td>round trip fares from pittsburgh to philadelp...</td>\n",
       "      <td>1000 डॉलर के तहत पिट्सबर्ग से फिलाडेल्फिया तक ...</td>\n",
       "      <td>1000 Daॉlara ke tahata piTsabarga se philADelp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4968</th>\n",
       "      <td>atis_airfare</td>\n",
       "      <td>what is the airfare for flights from denver t...</td>\n",
       "      <td>डेनवर से पिट्सबर्ग तक डेल्टा एयरलाइन पर उड़ानो...</td>\n",
       "      <td>Denavara se piTsabarga taka DelTA eyaralAina p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4969</th>\n",
       "      <td>atis_flight</td>\n",
       "      <td>do you have any flights from denver to baltim...</td>\n",
       "      <td>क्या आपके पास डलास के माध्यम से डेनवर से बाल्ट...</td>\n",
       "      <td>kyA Apake pAsa DalAsa ke mAdhyama se Denavara ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4970</th>\n",
       "      <td>atis_airline</td>\n",
       "      <td>which airlines fly into and out of denver</td>\n",
       "      <td>कौन सी एयरलाइंस डेनवर से बाहर और बाहर उड़ती है</td>\n",
       "      <td>kauna sI eyaralAiMsa Denavara se bAhara aura b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4971</th>\n",
       "      <td>atis_flight</td>\n",
       "      <td>does continental fly from boston to san franc...</td>\n",
       "      <td>डेनवर में एक स्टॉप के साथ बोस्टन से सैन फ्रांस...</td>\n",
       "      <td>Denavara meM eka sTaॉpa ke sAtha bosTana se sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4972</th>\n",
       "      <td>atis_flight</td>\n",
       "      <td>is there a delta flight from denver to san fr...</td>\n",
       "      <td>क्या डेनवर से सैन फ्रांसिस्को के लिए एक डेल्टा...</td>\n",
       "      <td>kyA Denavara se saina phrAMsisko ke lie eka De...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4973 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               INTENTS                                            QUERIES  \\\n",
       "0          atis_flight   i want to fly from boston at 838 am and arriv...   \n",
       "1          atis_flight   what flights are available from pittsburgh to...   \n",
       "2     atis_flight_time   what is the arrival time in san francisco for...   \n",
       "3         atis_airfare            cheapest airfare from tacoma to orlando   \n",
       "4         atis_airfare   round trip fares from pittsburgh to philadelp...   \n",
       "...                ...                                                ...   \n",
       "4968      atis_airfare   what is the airfare for flights from denver t...   \n",
       "4969       atis_flight   do you have any flights from denver to baltim...   \n",
       "4970      atis_airline          which airlines fly into and out of denver   \n",
       "4971       atis_flight   does continental fly from boston to san franc...   \n",
       "4972       atis_flight   is there a delta flight from denver to san fr...   \n",
       "\n",
       "                                        translated_text  \\\n",
       "0     मैं सुबह 838 बजे बोस्टन से उड़ान भरना चाहता हू...   \n",
       "1     गुरुवार सुबह पिट्सबर्ग से बाल्टीमोर तक कौन सी ...   \n",
       "2     वाशिंगटन छोड़कर 755 बजे उड़ान के लिए सैन फ्रां...   \n",
       "3          टैकोमा से ऑरलैंडो तक सबसे सस्ता विमान किराया   \n",
       "4     1000 डॉलर के तहत पिट्सबर्ग से फिलाडेल्फिया तक ...   \n",
       "...                                                 ...   \n",
       "4968  डेनवर से पिट्सबर्ग तक डेल्टा एयरलाइन पर उड़ानो...   \n",
       "4969  क्या आपके पास डलास के माध्यम से डेनवर से बाल्ट...   \n",
       "4970     कौन सी एयरलाइंस डेनवर से बाहर और बाहर उड़ती है   \n",
       "4971  डेनवर में एक स्टॉप के साथ बोस्टन से सैन फ्रांस...   \n",
       "4972  क्या डेनवर से सैन फ्रांसिस्को के लिए एक डेल्टा...   \n",
       "\n",
       "                                        Transliteration  \n",
       "0     maiM subaha 838 baje bosTana se u.DAna bharanA...  \n",
       "1     guruvAra subaha piTsabarga se bAlTImora taka k...  \n",
       "2     vAshiMgaTana Cho.Dakara 755 baje u.DAna ke lie...  \n",
       "3     TaikomA se ऑralaiMDo taka sabase sastA vimAna ...  \n",
       "4     1000 Daॉlara ke tahata piTsabarga se philADelp...  \n",
       "...                                                 ...  \n",
       "4968  Denavara se piTsabarga taka DelTA eyaralAina p...  \n",
       "4969  kyA Apake pAsa DalAsa ke mAdhyama se Denavara ...  \n",
       "4970  kauna sI eyaralAiMsa Denavara se bAhara aura b...  \n",
       "4971  Denavara meM eka sTaॉpa ke sAtha bosTana se sa...  \n",
       "4972  kyA Denavara se saina phrAMsisko ke lie eka De...  \n",
       "\n",
       "[4973 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fabf6c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df.index:\n",
    "    df.loc[i]['Transliteration']=df.loc[i]['Transliteration'].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "223c55da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_symbol(filename):\n",
    "    for symbol in ['*', '?', '%', '&', '$', '(', ')', '#', '^', '@', '!', '~', '-', '+', '=', \",\", \"'\", '\"',\"/\", \".\"]:\n",
    "        if symbol in filename:\n",
    "            filename = filename.replace(symbol, '')\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4664556",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df.index:\n",
    "    df.loc[i]['Transliteration']=replace_symbol(df.loc[i]['Transliteration'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e23362e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>INTENTS</th>\n",
       "      <th>QUERIES</th>\n",
       "      <th>translated_text</th>\n",
       "      <th>Transliteration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>atis_flight</td>\n",
       "      <td>i want to fly from boston at 838 am and arriv...</td>\n",
       "      <td>मैं सुबह 838 बजे बोस्टन से उड़ान भरना चाहता हू...</td>\n",
       "      <td>maim subaha 838 baje bostana se udana bharana ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>atis_flight</td>\n",
       "      <td>what flights are available from pittsburgh to...</td>\n",
       "      <td>गुरुवार सुबह पिट्सबर्ग से बाल्टीमोर तक कौन सी ...</td>\n",
       "      <td>guruvara subaha pitsabarga se baltimora taka k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>atis_flight_time</td>\n",
       "      <td>what is the arrival time in san francisco for...</td>\n",
       "      <td>वाशिंगटन छोड़कर 755 बजे उड़ान के लिए सैन फ्रां...</td>\n",
       "      <td>vashimgatana chodakara 755 baje udana ke lie s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>atis_airfare</td>\n",
       "      <td>cheapest airfare from tacoma to orlando</td>\n",
       "      <td>टैकोमा से ऑरलैंडो तक सबसे सस्ता विमान किराया</td>\n",
       "      <td>taikoma se ऑralaimdo taka sabase sasta vimana ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>atis_airfare</td>\n",
       "      <td>round trip fares from pittsburgh to philadelp...</td>\n",
       "      <td>1000 डॉलर के तहत पिट्सबर्ग से फिलाडेल्फिया तक ...</td>\n",
       "      <td>1000 daॉlara ke tahata pitsabarga se philadelp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4968</th>\n",
       "      <td>atis_airfare</td>\n",
       "      <td>what is the airfare for flights from denver t...</td>\n",
       "      <td>डेनवर से पिट्सबर्ग तक डेल्टा एयरलाइन पर उड़ानो...</td>\n",
       "      <td>denavara se pitsabarga taka delta eyaralaina p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4969</th>\n",
       "      <td>atis_flight</td>\n",
       "      <td>do you have any flights from denver to baltim...</td>\n",
       "      <td>क्या आपके पास डलास के माध्यम से डेनवर से बाल्ट...</td>\n",
       "      <td>kya apake pasa dalasa ke madhyama se denavara ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4970</th>\n",
       "      <td>atis_airline</td>\n",
       "      <td>which airlines fly into and out of denver</td>\n",
       "      <td>कौन सी एयरलाइंस डेनवर से बाहर और बाहर उड़ती है</td>\n",
       "      <td>kauna si eyaralaimsa denavara se bahara aura b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4971</th>\n",
       "      <td>atis_flight</td>\n",
       "      <td>does continental fly from boston to san franc...</td>\n",
       "      <td>डेनवर में एक स्टॉप के साथ बोस्टन से सैन फ्रांस...</td>\n",
       "      <td>denavara mem eka staॉpa ke satha bostana se sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4972</th>\n",
       "      <td>atis_flight</td>\n",
       "      <td>is there a delta flight from denver to san fr...</td>\n",
       "      <td>क्या डेनवर से सैन फ्रांसिस्को के लिए एक डेल्टा...</td>\n",
       "      <td>kya denavara se saina phramsisko ke lie eka de...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4973 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               INTENTS                                            QUERIES  \\\n",
       "0          atis_flight   i want to fly from boston at 838 am and arriv...   \n",
       "1          atis_flight   what flights are available from pittsburgh to...   \n",
       "2     atis_flight_time   what is the arrival time in san francisco for...   \n",
       "3         atis_airfare            cheapest airfare from tacoma to orlando   \n",
       "4         atis_airfare   round trip fares from pittsburgh to philadelp...   \n",
       "...                ...                                                ...   \n",
       "4968      atis_airfare   what is the airfare for flights from denver t...   \n",
       "4969       atis_flight   do you have any flights from denver to baltim...   \n",
       "4970      atis_airline          which airlines fly into and out of denver   \n",
       "4971       atis_flight   does continental fly from boston to san franc...   \n",
       "4972       atis_flight   is there a delta flight from denver to san fr...   \n",
       "\n",
       "                                        translated_text  \\\n",
       "0     मैं सुबह 838 बजे बोस्टन से उड़ान भरना चाहता हू...   \n",
       "1     गुरुवार सुबह पिट्सबर्ग से बाल्टीमोर तक कौन सी ...   \n",
       "2     वाशिंगटन छोड़कर 755 बजे उड़ान के लिए सैन फ्रां...   \n",
       "3          टैकोमा से ऑरलैंडो तक सबसे सस्ता विमान किराया   \n",
       "4     1000 डॉलर के तहत पिट्सबर्ग से फिलाडेल्फिया तक ...   \n",
       "...                                                 ...   \n",
       "4968  डेनवर से पिट्सबर्ग तक डेल्टा एयरलाइन पर उड़ानो...   \n",
       "4969  क्या आपके पास डलास के माध्यम से डेनवर से बाल्ट...   \n",
       "4970     कौन सी एयरलाइंस डेनवर से बाहर और बाहर उड़ती है   \n",
       "4971  डेनवर में एक स्टॉप के साथ बोस्टन से सैन फ्रांस...   \n",
       "4972  क्या डेनवर से सैन फ्रांसिस्को के लिए एक डेल्टा...   \n",
       "\n",
       "                                        Transliteration  \n",
       "0     maim subaha 838 baje bostana se udana bharana ...  \n",
       "1     guruvara subaha pitsabarga se baltimora taka k...  \n",
       "2     vashimgatana chodakara 755 baje udana ke lie s...  \n",
       "3     taikoma se ऑralaimdo taka sabase sasta vimana ...  \n",
       "4     1000 daॉlara ke tahata pitsabarga se philadelp...  \n",
       "...                                                 ...  \n",
       "4968  denavara se pitsabarga taka delta eyaralaina p...  \n",
       "4969  kya apake pasa dalasa ke madhyama se denavara ...  \n",
       "4970  kauna si eyaralaimsa denavara se bahara aura b...  \n",
       "4971  denavara mem eka staॉpa ke satha bostana se sa...  \n",
       "4972  kya denavara se saina phramsisko ke lie eka de...  \n",
       "\n",
       "[4973 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23479169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !Pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bf0c059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of news: 4973\n",
      "----------------------------------------\n",
      "Split by category:\n",
      "atis_flight                    3666\n",
      "atis_airfare                    423\n",
      "atis_ground_service             255\n",
      "atis_airline                    157\n",
      "atis_abbreviation               147\n",
      "atis_aircraft                    81\n",
      "atis_flight_time                 54\n",
      "atis_quantity                    51\n",
      "atis_flight#atis_airfare         20\n",
      "atis_airport                     20\n",
      "atis_distance                    20\n",
      "atis_city                        19\n",
      "atis_ground_fare                 18\n",
      "atis_capacity                    16\n",
      "atis_flight_no                   12\n",
      "atis_meal                         6\n",
      "atis_restriction                  6\n",
      "atis_airline#atis_flight_no       2\n",
      "Name: INTENTS, dtype: int64\n",
      "----------------------------------------\n",
      "Number of categories: 18\n"
     ]
    }
   ],
   "source": [
    "print('Total number of news: {}'.format(len(df)))\n",
    "print(40*'-')\n",
    "print('Split by category:')\n",
    "print(df[\"INTENTS\"].value_counts())\n",
    "print(40*'-')\n",
    "nr_categories = len(df[\"INTENTS\"].unique())\n",
    "print(\"Number of categories: {n}\".format(n=nr_categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd7d5590",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = df['Transliteration']\n",
    "y = df['INTENTS']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=df['INTENTS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef7c0eb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer(max_features=15000, ngram_range=(1, 2))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(max_features=15000, ngram_range=(1, 2))</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TfidfVectorizer(max_features=15000, ngram_range=(1, 2))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = X_train\n",
    "# Initizalize the vectorizer with max nr words and ngrams (1: single words, 2: two words in a row)\n",
    "vectorizer_tfidf = TfidfVectorizer(max_features=15000, ngram_range=(1,2))\n",
    "# Fit the vectorizer to the training data\n",
    "vectorizer_tfidf.fit(corpus)\n",
    "TfidfVectorizer(max_features=15000, ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03d790ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                 TfidfVectorizer(max_features=15000, ngram_range=(1, 2))),\n",
       "                (&#x27;classifier&#x27;, LogisticRegression())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                 TfidfVectorizer(max_features=15000, ngram_range=(1, 2))),\n",
       "                (&#x27;classifier&#x27;, LogisticRegression())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(max_features=15000, ngram_range=(1, 2))</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('vectorizer',\n",
       "                 TfidfVectorizer(max_features=15000, ngram_range=(1, 2))),\n",
       "                ('classifier', LogisticRegression())])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "import datetime\n",
    "classifier_tfidf = LogisticRegression()\n",
    "model_tfidf = Pipeline([(\"vectorizer\", vectorizer_tfidf), (\"classifier\", classifier_tfidf)])\n",
    "model_tfidf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "278df85a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Training data: 91.8%\n",
      "Accuracy Test data: 89.1%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "predicted_train_tfidf = model_tfidf.predict(X_train)\n",
    "accuracy_train_tfidf = accuracy_score(y_train, predicted_train_tfidf)\n",
    "print('Accuracy Training data: {:.1%}'.format(accuracy_train_tfidf))\n",
    "\n",
    "predicted_test_tfidf = model_tfidf.predict(X_test)\n",
    "accuracy_test_tfidf = accuracy_score(y_test, predicted_test_tfidf)\n",
    "accuracy_tfidf = accuracy_test_tfidf\n",
    "print('Accuracy Test data: {:.1%}'.format(accuracy_test_tfidf))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32067f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "import datetime\n",
    "classifier_tfidf = LogisticRegression()\n",
    "model_tfidf = Pipeline([(\"vectorizer\", vectorizer_tfidf), (\"classifier\", classifier_tfidf)])\n",
    "model_tfidf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e405622c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "predicted_train_tfidf = model_tfidf.predict(X_train)\n",
    "accuracy_train_tfidf = accuracy_score(y_train, predicted_train_tfidf)\n",
    "print('Accuracy Training data: {:.1%}'.format(accuracy_train_tfidf))\n",
    "\n",
    "predicted_test_tfidf = model_tfidf.predict(X_test)\n",
    "accuracy_test_tfidf = accuracy_score(y_test, predicted_test_tfidf)\n",
    "accuracy_tfidf = accuracy_test_tfidf\n",
    "print('Accuracy Test data: {:.1%}'.format(accuracy_test_tfidf))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "02f603ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes of the model:  ['atis_abbreviation' 'atis_aircraft' 'atis_airfare' 'atis_airline'\n",
      " 'atis_airline#atis_flight_no' 'atis_airport' 'atis_capacity' 'atis_city'\n",
      " 'atis_distance' 'atis_flight' 'atis_flight#atis_airfare' 'atis_flight_no'\n",
      " 'atis_flight_time' 'atis_ground_fare' 'atis_ground_service' 'atis_meal'\n",
      " 'atis_quantity' 'atis_restriction']\n",
      "--------------------------------------------------------------------------------\n",
      "Shape of the coefficients of the model (categories x vocabulary size):  (18, 7106)\n",
      "--------------------------------------------------------------------------------\n",
      "atis_abbreviation: kya matalaba, kiraya, matalaba hai, matalaba, ka kya, hai, kya hai, kya, kiraya koda, koda, \n",
      "--------------------------------------------------------------------------------\n",
      "atis_aircraft: taraha, kiya, vimana ka, kisa taraha, ka, upayoga, ka upayoga, kisa, prakara, vimana, \n",
      "--------------------------------------------------------------------------------\n",
      "atis_airfare: ki lagata, lagata, kirae, taka, sasta, sabase sasta, ka kiraya, kiraya kya, tikata, kiraya, \n",
      "--------------------------------------------------------------------------------\n",
      "atis_airline: udati haim, kya eyaralaina, eyaralaimsa dikhao, kisa eyaralaimsa, si eyaralaimsa, udati, eyaralaimsa dikhaem, kya eyaralaimsa, eyaralaina, eyaralaimsa, \n",
      "--------------------------------------------------------------------------------\n",
      "atis_airline#atis_flight_no: udana samkhya, toramto taka, eyaralaimsa aura, dekha, dekha sakata, aura udana, krripaya nyuya, maim krripaya, taka eyaralaimsa, samkhya dekha, \n",
      "--------------------------------------------------------------------------------\n",
      "atis_airport: mem, rka mem, nama, havai addom, addom, adde ka, adde, havai adde, mem havai, havai, \n",
      "--------------------------------------------------------------------------------\n",
      "atis_capacity: phita, baithane ki, baithane, mem kitani, kitane, sitem, kitani sitem, ki kshamata, kshamata, kshamata kya, \n",
      "--------------------------------------------------------------------------------\n",
      "atis_city: sthita hai, imtaraneshanala sthita, jaham janarala, sthita, seva, shahara, imtaraneshanala, dvara, shaharom ko, shaharom, \n",
      "--------------------------------------------------------------------------------\n",
      "atis_distance: adde se, havai adde, adde, dura hai, kitani dura, dura, havai, duri, ki duri, duri kya, \n",
      "--------------------------------------------------------------------------------\n",
      "atis_flight: ki udanem, ko, kya udanem, eka udana, udanom ko, udana kya, udanem dikhaem, se, udana, udanem, \n",
      "--------------------------------------------------------------------------------\n",
      "atis_flight#atis_airfare: 1100 da, kama 1100, sabhi, kama, sabhi udanem, se kama, aura, kirae, aura kirae, udanem aura, \n",
      "--------------------------------------------------------------------------------\n",
      "atis_flight_no: udanom ki, kala kolambasa, udana, miniyapolisa, kolambasa, se miniyapolisa, samkhya kya, ki samkhya, udana samkhya, samkhya, \n",
      "--------------------------------------------------------------------------------\n",
      "atis_flight_time: karyakrama, prasthana, anusuchi, samaya ko, udana ke, ka, ka samaya, udanom ka, ke samaya, samaya, \n",
      "--------------------------------------------------------------------------------\n",
      "atis_ground_fare: kirae para, lene, para lene, lagata kya, lagata, eka limosina, seva, ki lagata, limosina seva, limosina, \n",
      "--------------------------------------------------------------------------------\n",
      "atis_ground_service: adde, mem graumda, upalabdha hai, mem, jamini, jamini parivahana, graumda tramsaporteshana, tramsaporteshana, graumda, parivahana, \n",
      "--------------------------------------------------------------------------------\n",
      "atis_meal: mujhe atalamta, bhojana dikhaem, sabhi bhojana, ke bhojana, bhojana upalabdha, upalabdha, upalabdha bhojana, bhojana kya, sabhi upalabdha, bhojana, \n",
      "--------------------------------------------------------------------------------\n",
      "atis_quantity: prathama shreni, prathama, se kitani, taka kitani, shreni ki, udanem haim, haim, kitani udanem, kitane, kitani, \n",
      "--------------------------------------------------------------------------------\n",
      "atis_restriction: sabase saste, para pratibamdha, kiraya para, saste eka, kya hai, pratibamdha kya, ap57, pratibamdha ap57, ap57 kya, pratibamdha, \n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print('Classes of the model: ',classifier_tfidf.classes_)\n",
    "print(80*'-')\n",
    "print('Shape of the coefficients of the model (categories x vocabulary size): ',classifier_tfidf.coef_.shape)\n",
    "print(80*'-')\n",
    "NN = 10\n",
    "# Get the 10 (here: NN, which you can adjust yourself) ids of the words with highest weights per category\n",
    "top_words = np.argsort(classifier_tfidf.coef_,axis=1)[:,-NN:]\n",
    "\n",
    "# Get the vocabulary of the model (mapping of words to ids):\n",
    "voc = vectorizer_tfidf.vocabulary_\n",
    "# Get the inverse vocabulary to map the ids of the words to the words:\n",
    "inv_voc = {v: k for k, v in voc.items()}\n",
    "\n",
    "# Get for each category (=class) the top ten words\n",
    "for n, w in enumerate(classifier_tfidf.classes_):\n",
    "    t = w + ': '\n",
    "    for i in range(NN):\n",
    "        t += inv_voc[top_words[n,i]]\n",
    "        if i!=NN:\n",
    "            t+=', '\n",
    "    print(t)\n",
    "    print(80*'-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ef452f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coding of labels into a one-hot vector: atis_flight is  [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "X_tf = df['Transliteration']\n",
    "y_tf_class = df['INTENTS']\n",
    "\n",
    "# Convert labels into a one-hot vector of size 5 (the number of distinct labels)\n",
    "lab = LabelBinarizer()\n",
    "lab.fit(y_tf_class)\n",
    "y_tf = lab.transform(y_tf_class)\n",
    "\n",
    "# Example (you can modify n)\n",
    "# n=100\n",
    "# print('Coding of labels into a one-hot vector: ' + y_tf_class[n] + ' is ', y_tf[n])\n",
    "\n",
    "# Split into training and test data\n",
    "X_tf_train, X_tf_test, y_tf_train, y_tf_test = train_test_split(X_tf, y_tf, test_size=0.3, random_state=42, stratify=df['INTENTS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b7fa0123",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "corpus = X_tf_train\n",
    "max_words = 1026\n",
    "tokenizer = tensorflow.keras.preprocessing.text.Tokenizer(lower=True, split=' ', num_words=max_words, oov_token=\"<pad>\", filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "voc = tokenizer.word_index\n",
    "reverse_voc = dict([(value, key) for (key, value) in voc.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4b54a34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 1000\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(X_tf_train)\n",
    "X_tf_train_seq = tensorflow.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "# Apply the same to test data\n",
    "X_tf_test_seq = tensorflow.keras.preprocessing.sequence.pad_sequences(tokenizer.texts_to_sequences(X_tf_test), maxlen=max_len)\n",
    "\n",
    "# n=10 # You can adjust n\n",
    "# print('Shape: ',X_tf_train_seq.shape)\n",
    "# print(100*'-')\n",
    "# print('Example: ',X_tf_train_seq[n,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d7d936ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\chinnu\\anaconda3\\lib\\site-packages (4.1.2)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\chinnu\\appdata\\roaming\\python\\python39\\site-packages (from gensim) (1.22.4)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\chinnu\\anaconda3\\lib\\site-packages (from gensim) (1.7.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\chinnu\\anaconda3\\lib\\site-packages (from gensim) (6.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cbbb5ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Replace 'path_to_model' with the actual file path to the downloaded Google Word2Vec model.\n",
    "path_to_model = 'GoogleNews-vectors-negative300.bin'\n",
    "w2v  = KeyedVectors.load_word2vec_format(path_to_model, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c43b2047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import Word2Vec\n",
    "# model = gensim.models.KeyedVectors.load_word2vec_format('./data/GoogleNews-vectors-negative300.bin.gz', binary=True)  \n",
    "# # Example of a word representation:\n",
    "# # w2v['hello'].shape\n",
    "# # (300,)\n",
    "# # Build weights of the embbeddings matrix using w2v\n",
    "emb_matrix=np.zeros((max_words+1, 300))\n",
    "for i in range(max_words):\n",
    "    w = reverse_voc[i+1]\n",
    "    if w in w2v:\n",
    "        emb_matrix[i+1,:] = w2v[w]\n",
    "emb_size = emb_matrix.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "078ce80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a word representation:\n",
    "w2v['hello'].shape\n",
    "(300,)\n",
    "# Build weights of the embbeddings matrix using w2v\n",
    "emb_matrix=np.zeros((max_words+1, 300))\n",
    "for i in range(max_words):\n",
    "    if(i!=1026):w = reverse_voc[i+1]\n",
    "    if w in w2v:\n",
    "        emb_matrix[i+1,:] = w2v[w]\n",
    "emb_size = emb_matrix.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6cd7580e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "input_ = keras.layers.Input(shape = X_tf_train_seq[0,:].shape,name='input')\n",
    "# Embedding layer (voc size plus un (\"UNK\", word with index 0)), using the pre-trained emb_matrix obtained from Word2Vec\n",
    "x = keras.layers.Embedding(max_words+1,emb_size,weights=[emb_matrix],trainable=False, name='embedding')(input_)\n",
    "x = keras.layers.Bidirectional(keras.layers.LSTM(15,dropout=0.2),name='bidirectional-lstm')(x) # LSTM layer\n",
    "x = keras.layers.Dropout(0.2, name='dropout')(x)\n",
    "x = keras.layers.Dense(64, activation='relu', name='dense')(x)\n",
    "output = keras.layers.Dense(nr_categories,activation='softmax', name='classification')(x)\n",
    "\n",
    "#model = models.Model(input_, output)\n",
    "model = keras.Model(input_, output)\n",
    "opt = keras.optimizers.Adam(learning_rate=0.01, beta_1=0.9)\n",
    "model.compile(optimizer=opt,loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d197f05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 1000)]            0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 1000, 300)         308100    \n",
      "                                                                 \n",
      " bidirectional-lstm (Bidire  (None, 30)                37920     \n",
      " ctional)                                                        \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 30)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                1984      \n",
      "                                                                 \n",
      " classification (Dense)      (None, 18)                1170      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 349174 (1.33 MB)\n",
      "Trainable params: 41074 (160.45 KB)\n",
      "Non-trainable params: 308100 (1.18 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "86d955be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "55/55 [==============================] - 82s 1s/step - loss: 1.2570 - accuracy: 0.7225 - val_loss: 1.0717 - val_accuracy: 0.7373\n",
      "Epoch 2/10\n",
      "55/55 [==============================] - 68s 1s/step - loss: 1.0409 - accuracy: 0.7371 - val_loss: 0.9646 - val_accuracy: 0.7373\n",
      "Epoch 3/10\n",
      "55/55 [==============================] - 76s 1s/step - loss: 0.9231 - accuracy: 0.7575 - val_loss: 0.8871 - val_accuracy: 0.7688\n",
      "Epoch 4/10\n",
      "55/55 [==============================] - 78s 1s/step - loss: 0.8720 - accuracy: 0.7716 - val_loss: 0.8422 - val_accuracy: 0.7661\n",
      "Epoch 5/10\n",
      "55/55 [==============================] - 80s 1s/step - loss: 0.8385 - accuracy: 0.7733 - val_loss: 0.8144 - val_accuracy: 0.7909\n",
      "Epoch 6/10\n",
      "55/55 [==============================] - 80s 1s/step - loss: 0.8091 - accuracy: 0.7883 - val_loss: 0.7899 - val_accuracy: 0.7942\n",
      "Epoch 7/10\n",
      "55/55 [==============================] - 80s 1s/step - loss: 0.7820 - accuracy: 0.7883 - val_loss: 0.8133 - val_accuracy: 0.8023\n",
      "Epoch 8/10\n",
      "55/55 [==============================] - 79s 1s/step - loss: 0.7619 - accuracy: 0.7960 - val_loss: 0.7570 - val_accuracy: 0.8063\n",
      "Epoch 9/10\n",
      "55/55 [==============================] - 78s 1s/step - loss: 0.7433 - accuracy: 0.8024 - val_loss: 0.7436 - val_accuracy: 0.8123\n",
      "Epoch 10/10\n",
      "55/55 [==============================] - 81s 1s/step - loss: 0.7145 - accuracy: 0.8061 - val_loss: 0.7312 - val_accuracy: 0.8150\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_tf_train_seq, y_tf_train, batch_size=64, shuffle=True, epochs=10, validation_data=(X_tf_test_seq, y_tf_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8691f9b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6124e1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Training data: 80.6%\n",
      "Accuracy Test data: 81.5%\n"
     ]
    }
   ],
   "source": [
    "accuracy_rnn = history.history['val_accuracy'][-1]\n",
    "print('Accuracy Training data: {:.1%}'.format(history.history['accuracy'][-1]))\n",
    "print('Accuracy Test data: {:.1%}'.format(history.history['val_accuracy'][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "aa6842ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40ab1c589eeb449eb2474c79c9640dbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chinnu\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:137: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\chinnu\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d30482a0ae394bb38db3241ed81453c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99d3e0e8911d46a29eb8fd25948d4fe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f26cfa9d87f64fa3a03b3842a6f11efb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import transformers\n",
    "X_bert = df['Transliteration']\n",
    "y_bert_class = df['INTENTS']\n",
    "\n",
    "# Convert labels into a one-hot vector of size 5 (the number of distinct labels)\n",
    "lab = LabelBinarizer()\n",
    "lab.fit(y_bert_class)\n",
    "y_bert = lab.transform(y_bert_class)\n",
    "\n",
    "# Example (you can modify n)\n",
    "# n=100\n",
    "# print('Coding of labels into a one-hot vector: ' + y_bert_class[n] + ' is ', y_bert[n])\n",
    "# Coding of labels into a one-hot vector: entertainment is  [0 1 0 0 0]\n",
    "# distil-bert tokenizer\n",
    "tokenizer_bert = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "X_bert = [tokenizer_bert(text, padding='max_length', max_length = 512, truncation=True)['input_ids'] for text in X_bert]\n",
    "X_bert = np.array(X_bert, dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "57f3ec2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bert_train, X_bert_test, y_bert_train, y_bert_test = train_test_split(X_bert, y_bert, test_size=0.3, random_state=42, stratify=df['INTENTS'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "78d59888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6322cc15926f4da8924849f7e8c67506",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "config = transformers.DistilBertConfig(dropout=0.2, attention_dropout=0.2)\n",
    "dbert = transformers.TFDistilBertModel.from_pretrained('distilbert-base-uncased', config=config, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "63046428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_token (InputLayer)    [(None, 512)]             0         \n",
      "                                                                 \n",
      " tf_distil_bert_model (TFDi  TFBaseModelOutput(last_   66362880  \n",
      " stilBertModel)              hidden_state=(None, 512             \n",
      "                             , 768),                             \n",
      "                              hidden_states=None, at             \n",
      "                             tentions=None)                      \n",
      "                                                                 \n",
      " tf.__operators__.getitem_2  (None, 768)               0         \n",
      "  (SlicingOpLambda)                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 768)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                49216     \n",
      "                                                                 \n",
      " classification (Dense)      (None, 18)                1170      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 66413266 (253.35 MB)\n",
      "Trainable params: 50386 (196.82 KB)\n",
      "Non-trainable params: 66362880 (253.15 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_ids_in = keras.layers.Input(shape=(512,), name='input_token', dtype='int32')\n",
    "\n",
    "x = dbert(input_ids=input_ids_in)[0][:,0,:]\n",
    "x = keras.layers.Dropout(0.2, name='dropout')(x)\n",
    "x = keras.layers.Dense(64, activation='relu', name='dense')(x)\n",
    "x = keras.layers.Dense(18, activation='softmax', name='classification')(x)\n",
    "\n",
    "dmodel = keras.Model(inputs=input_ids_in, outputs = x)\n",
    "\n",
    "dmodel.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "dmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a95081fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "109/109 [==============================] - 2000s 18s/step - loss: 1.2151 - accuracy: 0.7265 - val_loss: 1.1262 - val_accuracy: 0.7373\n",
      "Epoch 2/5\n",
      "109/109 [==============================] - 1571s 14s/step - loss: 1.1442 - accuracy: 0.7371 - val_loss: 1.1081 - val_accuracy: 0.7373\n",
      "Epoch 3/5\n",
      "109/109 [==============================] - 1801s 17s/step - loss: 1.1257 - accuracy: 0.7371 - val_loss: 1.2452 - val_accuracy: 0.7373\n",
      "Epoch 4/5\n",
      "109/109 [==============================] - 2089s 19s/step - loss: 1.1287 - accuracy: 0.7371 - val_loss: 1.1326 - val_accuracy: 0.7373\n",
      "Epoch 5/5\n",
      "109/109 [==============================] - 1582s 15s/step - loss: 1.1211 - accuracy: 0.7369 - val_loss: 1.1467 - val_accuracy: 0.7373\n"
     ]
    }
   ],
   "source": [
    "history = dmodel.fit(X_bert_train, y_bert_train, batch_size=32, shuffle=True, epochs=5, validation_data=(X_bert_test, y_bert_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb368eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('intentclass.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56632fae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>INTENTS</th>\n",
       "      <th>QUERIES</th>\n",
       "      <th>translated_text</th>\n",
       "      <th>Transliteration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>atis_flight</td>\n",
       "      <td>i want to fly from boston at 838 am and arriv...</td>\n",
       "      <td>मैं सुबह 838 बजे बोस्टन से उड़ान भरना चाहता हू...</td>\n",
       "      <td>maiM subaha 838 baje bosTana se u.DAna bharanA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>atis_flight</td>\n",
       "      <td>what flights are available from pittsburgh to...</td>\n",
       "      <td>गुरुवार सुबह पिट्सबर्ग से बाल्टीमोर तक कौन सी ...</td>\n",
       "      <td>guruvAra subaha piTsabarga se bAlTImora taka k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>atis_flight_time</td>\n",
       "      <td>what is the arrival time in san francisco for...</td>\n",
       "      <td>वाशिंगटन छोड़कर 755 बजे उड़ान के लिए सैन फ्रां...</td>\n",
       "      <td>vAshiMgaTana Cho.Dakara 755 baje u.DAna ke lie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>atis_airfare</td>\n",
       "      <td>cheapest airfare from tacoma to orlando</td>\n",
       "      <td>टैकोमा से ऑरलैंडो तक सबसे सस्ता विमान किराया</td>\n",
       "      <td>TaikomA se ऑralaiMDo taka sabase sastA vimAna ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>atis_airfare</td>\n",
       "      <td>round trip fares from pittsburgh to philadelp...</td>\n",
       "      <td>1000 डॉलर के तहत पिट्सबर्ग से फिलाडेल्फिया तक ...</td>\n",
       "      <td>1000 Daॉlara ke tahata piTsabarga se philADelp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4968</th>\n",
       "      <td>atis_airfare</td>\n",
       "      <td>what is the airfare for flights from denver t...</td>\n",
       "      <td>डेनवर से पिट्सबर्ग तक डेल्टा एयरलाइन पर उड़ानो...</td>\n",
       "      <td>Denavara se piTsabarga taka DelTA eyaralAina p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4969</th>\n",
       "      <td>atis_flight</td>\n",
       "      <td>do you have any flights from denver to baltim...</td>\n",
       "      <td>क्या आपके पास डलास के माध्यम से डेनवर से बाल्ट...</td>\n",
       "      <td>kyA Apake pAsa DalAsa ke mAdhyama se Denavara ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4970</th>\n",
       "      <td>atis_airline</td>\n",
       "      <td>which airlines fly into and out of denver</td>\n",
       "      <td>कौन सी एयरलाइंस डेनवर से बाहर और बाहर उड़ती है</td>\n",
       "      <td>kauna sI eyaralAiMsa Denavara se bAhara aura b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4971</th>\n",
       "      <td>atis_flight</td>\n",
       "      <td>does continental fly from boston to san franc...</td>\n",
       "      <td>डेनवर में एक स्टॉप के साथ बोस्टन से सैन फ्रांस...</td>\n",
       "      <td>Denavara meM eka sTaॉpa ke sAtha bosTana se sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4972</th>\n",
       "      <td>atis_flight</td>\n",
       "      <td>is there a delta flight from denver to san fr...</td>\n",
       "      <td>क्या डेनवर से सैन फ्रांसिस्को के लिए एक डेल्टा...</td>\n",
       "      <td>kyA Denavara se saina phrAMsisko ke lie eka De...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4973 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               INTENTS                                            QUERIES  \\\n",
       "0          atis_flight   i want to fly from boston at 838 am and arriv...   \n",
       "1          atis_flight   what flights are available from pittsburgh to...   \n",
       "2     atis_flight_time   what is the arrival time in san francisco for...   \n",
       "3         atis_airfare            cheapest airfare from tacoma to orlando   \n",
       "4         atis_airfare   round trip fares from pittsburgh to philadelp...   \n",
       "...                ...                                                ...   \n",
       "4968      atis_airfare   what is the airfare for flights from denver t...   \n",
       "4969       atis_flight   do you have any flights from denver to baltim...   \n",
       "4970      atis_airline          which airlines fly into and out of denver   \n",
       "4971       atis_flight   does continental fly from boston to san franc...   \n",
       "4972       atis_flight   is there a delta flight from denver to san fr...   \n",
       "\n",
       "                                        translated_text  \\\n",
       "0     मैं सुबह 838 बजे बोस्टन से उड़ान भरना चाहता हू...   \n",
       "1     गुरुवार सुबह पिट्सबर्ग से बाल्टीमोर तक कौन सी ...   \n",
       "2     वाशिंगटन छोड़कर 755 बजे उड़ान के लिए सैन फ्रां...   \n",
       "3          टैकोमा से ऑरलैंडो तक सबसे सस्ता विमान किराया   \n",
       "4     1000 डॉलर के तहत पिट्सबर्ग से फिलाडेल्फिया तक ...   \n",
       "...                                                 ...   \n",
       "4968  डेनवर से पिट्सबर्ग तक डेल्टा एयरलाइन पर उड़ानो...   \n",
       "4969  क्या आपके पास डलास के माध्यम से डेनवर से बाल्ट...   \n",
       "4970     कौन सी एयरलाइंस डेनवर से बाहर और बाहर उड़ती है   \n",
       "4971  डेनवर में एक स्टॉप के साथ बोस्टन से सैन फ्रांस...   \n",
       "4972  क्या डेनवर से सैन फ्रांसिस्को के लिए एक डेल्टा...   \n",
       "\n",
       "                                        Transliteration  \n",
       "0     maiM subaha 838 baje bosTana se u.DAna bharanA...  \n",
       "1     guruvAra subaha piTsabarga se bAlTImora taka k...  \n",
       "2     vAshiMgaTana Cho.Dakara 755 baje u.DAna ke lie...  \n",
       "3     TaikomA se ऑralaiMDo taka sabase sastA vimAna ...  \n",
       "4     1000 Daॉlara ke tahata piTsabarga se philADelp...  \n",
       "...                                                 ...  \n",
       "4968  Denavara se piTsabarga taka DelTA eyaralAina p...  \n",
       "4969  kyA Apake pAsa DalAsa ke mAdhyama se Denavara ...  \n",
       "4970  kauna sI eyaralAiMsa Denavara se bAhara aura b...  \n",
       "4971  Denavara meM eka sTaॉpa ke sAtha bosTana se sa...  \n",
       "4972  kyA Denavara se saina phrAMsisko ke lie eka De...  \n",
       "\n",
       "[4973 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b98253d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\chinnu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from matplotlib import pyplot as plt\n",
    "import re \n",
    "import string \n",
    "import math\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "import scipy.stats\n",
    "\n",
    "\n",
    "# Removing the unwanted chacters and finding total number of words in each sentence.\n",
    "df['translated_text'] =  [re.sub(r'[?|$|.|!|।|,|:|;|]','', str(x)) for x in df['translated_text']]\n",
    "df['totalwords'] = df['translated_text'].str.split().str.len() \n",
    "\n",
    "#  Average number of words in each sentence.\n",
    "sum=df['totalwords'].sum() # total number of words in all sentences.\n",
    "n=df['totalwords'].count() # total number of sentences.\n",
    "aver_words=sum/n\n",
    "\n",
    "# Words tokenization of all sentences.\n",
    "lt = df[\"translated_text\"].tolist()\n",
    "joint_words = ' '.join(lt)\n",
    "tokenized_word=word_tokenize(joint_words)\n",
    "\n",
    "# Frequency distribution of all sentences.\n",
    "fdist = FreqDist(tokenized_word)\n",
    "\n",
    "# type token ratio\n",
    "l=len(fdist)\n",
    "type_token_ratio=(l/sum)*100\n",
    "\n",
    "# Removing stop words from each sentences\n",
    "stop_words =  set(open('stop_words.txt',encoding=\"utf8\").read().split())\n",
    "df['sentence_without_stopwords'] = df['translated_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da7d07f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'सुबह 838 बजे बोस्टन उड़ान भरना चाहता सुबह 1110 बजे डेनवर पहुंचना चाहता'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]['sentence_without_stopwords']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b91d50bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>INTENTS</th>\n",
       "      <th>QUERIES</th>\n",
       "      <th>translated_text</th>\n",
       "      <th>Transliteration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>atis_flight</td>\n",
       "      <td>i want to fly from boston at 838 am and arriv...</td>\n",
       "      <td>मैं सुबह 838 बजे बोस्टन से उड़ान भरना चाहता हू...</td>\n",
       "      <td>maim subaha 838 baje bostana se udana bharana ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>atis_flight</td>\n",
       "      <td>what flights are available from pittsburgh to...</td>\n",
       "      <td>गुरुवार सुबह पिट्सबर्ग से बाल्टीमोर तक कौन सी ...</td>\n",
       "      <td>guruvara subaha pitsabarga se baltimora taka k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>atis_flight_time</td>\n",
       "      <td>what is the arrival time in san francisco for...</td>\n",
       "      <td>वाशिंगटन छोड़कर 755 बजे उड़ान के लिए सैन फ्रां...</td>\n",
       "      <td>vashimgatana chodakara 755 baje udana ke lie s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>atis_airfare</td>\n",
       "      <td>cheapest airfare from tacoma to orlando</td>\n",
       "      <td>टैकोमा से ऑरलैंडो तक सबसे सस्ता विमान किराया</td>\n",
       "      <td>taikoma se ऑralaimdo taka sabase sasta vimana ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>atis_airfare</td>\n",
       "      <td>round trip fares from pittsburgh to philadelp...</td>\n",
       "      <td>1000 डॉलर के तहत पिट्सबर्ग से फिलाडेल्फिया तक ...</td>\n",
       "      <td>1000 daॉlara ke tahata pitsabarga se philadelp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4968</th>\n",
       "      <td>atis_airfare</td>\n",
       "      <td>what is the airfare for flights from denver t...</td>\n",
       "      <td>डेनवर से पिट्सबर्ग तक डेल्टा एयरलाइन पर उड़ानो...</td>\n",
       "      <td>denavara se pitsabarga taka delta eyaralaina p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4969</th>\n",
       "      <td>atis_flight</td>\n",
       "      <td>do you have any flights from denver to baltim...</td>\n",
       "      <td>क्या आपके पास डलास के माध्यम से डेनवर से बाल्ट...</td>\n",
       "      <td>kya apake pasa dalasa ke madhyama se denavara ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4970</th>\n",
       "      <td>atis_airline</td>\n",
       "      <td>which airlines fly into and out of denver</td>\n",
       "      <td>कौन सी एयरलाइंस डेनवर से बाहर और बाहर उड़ती है</td>\n",
       "      <td>kauna si eyaralaimsa denavara se bahara aura b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4971</th>\n",
       "      <td>atis_flight</td>\n",
       "      <td>does continental fly from boston to san franc...</td>\n",
       "      <td>डेनवर में एक स्टॉप के साथ बोस्टन से सैन फ्रांस...</td>\n",
       "      <td>denavara mem eka staॉpa ke satha bostana se sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4972</th>\n",
       "      <td>atis_flight</td>\n",
       "      <td>is there a delta flight from denver to san fr...</td>\n",
       "      <td>क्या डेनवर से सैन फ्रांसिस्को के लिए एक डेल्टा...</td>\n",
       "      <td>kya denavara se saina phramsisko ke lie eka de...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4973 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               INTENTS                                            QUERIES  \\\n",
       "0          atis_flight   i want to fly from boston at 838 am and arriv...   \n",
       "1          atis_flight   what flights are available from pittsburgh to...   \n",
       "2     atis_flight_time   what is the arrival time in san francisco for...   \n",
       "3         atis_airfare            cheapest airfare from tacoma to orlando   \n",
       "4         atis_airfare   round trip fares from pittsburgh to philadelp...   \n",
       "...                ...                                                ...   \n",
       "4968      atis_airfare   what is the airfare for flights from denver t...   \n",
       "4969       atis_flight   do you have any flights from denver to baltim...   \n",
       "4970      atis_airline          which airlines fly into and out of denver   \n",
       "4971       atis_flight   does continental fly from boston to san franc...   \n",
       "4972       atis_flight   is there a delta flight from denver to san fr...   \n",
       "\n",
       "                                        translated_text  \\\n",
       "0     मैं सुबह 838 बजे बोस्टन से उड़ान भरना चाहता हू...   \n",
       "1     गुरुवार सुबह पिट्सबर्ग से बाल्टीमोर तक कौन सी ...   \n",
       "2     वाशिंगटन छोड़कर 755 बजे उड़ान के लिए सैन फ्रां...   \n",
       "3          टैकोमा से ऑरलैंडो तक सबसे सस्ता विमान किराया   \n",
       "4     1000 डॉलर के तहत पिट्सबर्ग से फिलाडेल्फिया तक ...   \n",
       "...                                                 ...   \n",
       "4968  डेनवर से पिट्सबर्ग तक डेल्टा एयरलाइन पर उड़ानो...   \n",
       "4969  क्या आपके पास डलास के माध्यम से डेनवर से बाल्ट...   \n",
       "4970     कौन सी एयरलाइंस डेनवर से बाहर और बाहर उड़ती है   \n",
       "4971  डेनवर में एक स्टॉप के साथ बोस्टन से सैन फ्रांस...   \n",
       "4972  क्या डेनवर से सैन फ्रांसिस्को के लिए एक डेल्टा...   \n",
       "\n",
       "                                        Transliteration  \n",
       "0     maim subaha 838 baje bostana se udana bharana ...  \n",
       "1     guruvara subaha pitsabarga se baltimora taka k...  \n",
       "2     vashimgatana chodakara 755 baje udana ke lie s...  \n",
       "3     taikoma se ऑralaimdo taka sabase sasta vimana ...  \n",
       "4     1000 daॉlara ke tahata pitsabarga se philadelp...  \n",
       "...                                                 ...  \n",
       "4968  denavara se pitsabarga taka delta eyaralaina p...  \n",
       "4969  kya apake pasa dalasa ke madhyama se denavara ...  \n",
       "4970  kauna si eyaralaimsa denavara se bahara aura b...  \n",
       "4971  denavara mem eka staॉpa ke satha bostana se sa...  \n",
       "4972  kya denavara se saina phramsisko ke lie eka de...  \n",
       "\n",
       "[4973 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22721417",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "y=df['INTENTS']\n",
    "X=df['Transliteration']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Encoding y.\n",
    "Encoder = preprocessing.LabelEncoder()\n",
    "y_train = Encoder.fit_transform(y_train)\n",
    "y_test = Encoder.fit_transform(y_test)\n",
    "all_sentences = df['Transliteration'].tolist()\n",
    "# Finding TF-IDF for each word.\n",
    "Tfidf_vect = TfidfVectorizer(max_features=5000)\n",
    "Tfidf_vect.fit(all_sentences)\n",
    "X_train_Tfidf = Tfidf_vect.transform(X_train)\n",
    "X_test_Tfidf = Tfidf_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0e1ca1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e4227ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Applying support vector machine algorithm.\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=5, gamma='auto')\n",
    "SVM.fit(X_train_Tfidf,y_train)\n",
    "predictions_SVM = SVM.predict(X_test_Tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34ddf77c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.773869346733667"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Accuracy score support vector machine\n",
    "accuracy_score(predictions_SVM, y_test)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3824fcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Naive = MultinomialNB()\n",
    "Naive.fit(X_train_Tfidf,y_train)\n",
    "predictions_NB = Naive.predict(X_test_Tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4fc29da9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82.1105527638191"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(predictions_NB, y_test)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e67eb98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88.24120603015075"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(X_train_Tfidf,y_train)\n",
    "predictions_LR=clf.predict(X_test_Tfidf)\n",
    "accuracy_score(predictions_LR, y_test)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "358aa215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89.64824120603015"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgf =SGDClassifier(loss='modified_huber',shuffle=True,random_state=101)\n",
    "sgf.fit(X_train_Tfidf,y_train)\n",
    "predictions_SRD=sgf.predict(X_test_Tfidf)\n",
    "accuracy_score(predictions_SRD, y_test)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a1786fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree=DecisionTreeClassifier(max_depth=10,random_state=101,max_features=None,min_samples_leaf=15)\n",
    "dtree.fit(X_train_Tfidf,y_train)\n",
    "predictions_DT=dtree.predict(X_test_Tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a5ef00ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83.91959798994975"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(predictions_DT, y_test)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "584e7f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm=RandomForestClassifier(n_estimators=1000,oob_score=True,n_jobs=1,random_state=101,max_features=1000,min_samples_leaf=30)\n",
    "rfm.fit(X_train_Tfidf,y_train)\n",
    "predictions_RF=rfm.predict(X_test_Tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a030e54b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81.50753768844221"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(predictions_RF, y_test)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "287967cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes of the model:  ['atis_abbreviation' 'atis_aircraft' 'atis_airfare' 'atis_airline'\n",
      " 'atis_airline#atis_flight_no' 'atis_airport' 'atis_capacity' 'atis_city'\n",
      " 'atis_distance' 'atis_flight' 'atis_flight#atis_airfare' 'atis_flight_no'\n",
      " 'atis_flight_time' 'atis_ground_fare' 'atis_ground_service' 'atis_meal'\n",
      " 'atis_quantity' 'atis_restriction']\n",
      "--------------------------------------------------------------------------------\n",
      "Shape of the coefficients of the model (categories x vocabulary size):  (18, 7106)\n",
      "--------------------------------------------------------------------------------\n",
      "atis_abbreviation: kya matalaba, kiraya, matalaba hai, matalaba, ka kya, hai, kya hai, kya, kiraya koda, koda, \n",
      "--------------------------------------------------------------------------------\n",
      "atis_aircraft: taraha, kiya, vimana ka, kisa taraha, ka, upayoga, ka upayoga, kisa, prakara, vimana, \n",
      "--------------------------------------------------------------------------------\n",
      "atis_airfare: ki lagata, lagata, kirae, taka, sasta, sabase sasta, ka kiraya, kiraya kya, tikata, kiraya, \n",
      "--------------------------------------------------------------------------------\n",
      "atis_airline: udati haim, kya eyaralaina, eyaralaimsa dikhao, kisa eyaralaimsa, si eyaralaimsa, udati, eyaralaimsa dikhaem, kya eyaralaimsa, eyaralaina, eyaralaimsa, \n",
      "--------------------------------------------------------------------------------\n",
      "atis_airline#atis_flight_no: udana samkhya, toramto taka, eyaralaimsa aura, dekha, dekha sakata, aura udana, krripaya nyuya, maim krripaya, taka eyaralaimsa, samkhya dekha, \n",
      "--------------------------------------------------------------------------------\n",
      "atis_airport: mem, rka mem, nama, havai addom, addom, adde ka, adde, havai adde, mem havai, havai, \n",
      "--------------------------------------------------------------------------------\n",
      "atis_capacity: phita, baithane ki, baithane, mem kitani, kitane, sitem, kitani sitem, ki kshamata, kshamata, kshamata kya, \n",
      "--------------------------------------------------------------------------------\n",
      "atis_city: sthita hai, imtaraneshanala sthita, jaham janarala, sthita, seva, shahara, imtaraneshanala, dvara, shaharom ko, shaharom, \n",
      "--------------------------------------------------------------------------------\n",
      "atis_distance: adde se, havai adde, adde, dura hai, kitani dura, dura, havai, duri, ki duri, duri kya, \n",
      "--------------------------------------------------------------------------------\n",
      "atis_flight: ki udanem, ko, kya udanem, eka udana, udanom ko, udana kya, udanem dikhaem, se, udana, udanem, \n",
      "--------------------------------------------------------------------------------\n",
      "atis_flight#atis_airfare: 1100 da, kama 1100, sabhi, kama, sabhi udanem, se kama, aura, kirae, aura kirae, udanem aura, \n",
      "--------------------------------------------------------------------------------\n",
      "atis_flight_no: udanom ki, kala kolambasa, udana, miniyapolisa, kolambasa, se miniyapolisa, samkhya kya, ki samkhya, udana samkhya, samkhya, \n",
      "--------------------------------------------------------------------------------\n",
      "atis_flight_time: karyakrama, prasthana, anusuchi, samaya ko, udana ke, ka, ka samaya, udanom ka, ke samaya, samaya, \n",
      "--------------------------------------------------------------------------------\n",
      "atis_ground_fare: kirae para, lene, para lene, lagata kya, lagata, eka limosina, seva, ki lagata, limosina seva, limosina, \n",
      "--------------------------------------------------------------------------------\n",
      "atis_ground_service: adde, mem graumda, upalabdha hai, mem, jamini, jamini parivahana, graumda tramsaporteshana, tramsaporteshana, graumda, parivahana, \n",
      "--------------------------------------------------------------------------------\n",
      "atis_meal: mujhe atalamta, bhojana dikhaem, sabhi bhojana, ke bhojana, bhojana upalabdha, upalabdha, upalabdha bhojana, bhojana kya, sabhi upalabdha, bhojana, \n",
      "--------------------------------------------------------------------------------\n",
      "atis_quantity: prathama shreni, prathama, se kitani, taka kitani, shreni ki, udanem haim, haim, kitani udanem, kitane, kitani, \n",
      "--------------------------------------------------------------------------------\n",
      "atis_restriction: sabase saste, para pratibamdha, kiraya para, saste eka, kya hai, pratibamdha kya, ap57, pratibamdha ap57, ap57 kya, pratibamdha, \n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print('Classes of the model: ',classifier_tfidf.classes_)\n",
    "print(80*'-')\n",
    "print('Shape of the coefficients of the model (categories x vocabulary size): ',classifier_tfidf.coef_.shape)\n",
    "print(80*'-')\n",
    "NN = 10\n",
    "# Get the 10 (here: NN, which you can adjust yourself) ids of the words with highest weights per category\n",
    "top_words = np.argsort(classifier_tfidf.coef_,axis=1)[:,-NN:]\n",
    "\n",
    "# Get the vocabulary of the model (mapping of words to ids):\n",
    "voc = vectorizer_tfidf.vocabulary_\n",
    "# Get the inverse vocabulary to map the ids of the words to the words:\n",
    "inv_voc = {v: k for k, v in voc.items()}\n",
    "\n",
    "# Get for each category (=class) the top ten words\n",
    "for n, w in enumerate(classifier_tfidf.classes_):\n",
    "    t = w + ': '\n",
    "    for i in range(NN):\n",
    "        t += inv_voc[top_words[n,i]]\n",
    "        if i!=NN:\n",
    "            t+=', '\n",
    "    print(t)\n",
    "    print(80*'-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8378baff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
